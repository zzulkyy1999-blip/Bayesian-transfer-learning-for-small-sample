import numpy as np
from scipy.optimize import differential_evolution

def hypar_update(n, D, a, b, b_ts, u, v):
    if n == 0: 
        return v, u, a, b

    x_bar = np.mean(D)
    S = np.var(D, ddof=0) 

    v_new = v + n
    u_new = (v * u + n * x_bar) / (v + n)
    a_new = a + n / 2
    
    denom_term = (b / b_ts) + (n * S / 2) + (n * v * (x_bar - u) ** 2 / (2 * (n + v)))
    
    if denom_term == 0:
        b_new = b 
    else:
        b_new = 1.0 / denom_term

    return v_new, u_new, a_new, b_new

def mape(y_true, y_pred):
    """Mean Absolute Percentage Error."""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
class MultiSourceBayesianTransfer:
    def __init__(self, target_train, source_datasets, rho_map):
        self.target_train = target_train
        self.source_datasets = source_datasets
        self.source_names = list(source_datasets.keys())
        self.rho_map = rho_map
        self.source_means = []
        self.source_bts = []
        self.a = 10
        self.b_t = 1
        self.b_s = 1
        self.v_t = 10
        self.v_s = 10 
        for name in self.source_names:
            D_s = self.source_datasets[name]
            rho = self.rho_map.get(name, 0.9) # Default rho
            u_s = np.mean(D_s)
            self.source_means.append(u_s)
            b_ts = (1 - rho) * self.b_t * self.b_s
            self.source_bts.append(b_ts)
            
        self.source_means = np.array(self.source_means)
        self.source_bts = np.array(self.source_bts)

    def _predict_with_weights(self, weights, train_data):
        w_norm = weights / (np.sum(weights) + 1e-9)
        u_prior_mixed = np.sum(w_norm * self.source_means)
        b_ts_mixed = np.sum(w_norm * self.source_bts)
        n_t = len(train_data)
        if n_t == 0:
            return u_prior_mixed
        _, u_t_star, _, _ = hypar_update(
            n_t, train_data, 
            self.a, self.b_s, b_ts_mixed, 
            u_prior_mixed, self.v_t
        )
        return u_t_star

    def objective_function(self, weights):
       
        errors = []
        n = len(self.target_train)
        
        if n < 2:
            pred = self._predict_with_weights(weights, self.target_train)
            return mape(self.target_train, pred) # 这种情况下其实是拟合误差，不太好
            
        for i in range(n):
           
            val_sample = self.target_train[i]
            
            train_subset = np.delete(self.target_train, i)
           
            pred = self._predict_with_weights(weights, train_subset)
            
            errors.append(mape(val_sample, pred))
            
        return np.mean(errors)

    def optimize_weights_de(self):
       
        n_sources = len(self.source_names)
        bounds = [(0, 1)] * n_sources
        
        result = differential_evolution(
            self.objective_function, 
            bounds, 
            strategy='best1bin', 
            maxiter=50, 
            popsize=15, 
            tol=1e-4, 
            mutation=(0.5, 1), 
            recombination=0.7,
            seed=42
        )
        
        best_weights = result.x
       
        norm_weights = best_weights / (np.sum(best_weights) + 1e-9)
        return norm_weights, result.fun

    def predict(self, best_weights):
       
        return self._predict_with_weights(best_weights, self.target_train)

if __name__ == "__main__":
  
    target_data = {
        "Compressive": {"train": np.array([6100, 6302, 6210]), "test": 6288},
        "Failure":     {"train": np.array([5796, 5988, 5898]), "test": 5970},
        "Yield":       {"train": np.array([4864, 5010, 4992]), "test": 4968}
    }

    source_datasets_raw = {
        "L1": {"Compressive": [5176, 4982, 5008, 5082, 5044], "Failure": [4654, 4480, 4500, 4564, 4532], "Yield": [4369, 4232, 4138, 4320, 4174]},
        "L2": {"Compressive": [7318, 7446, 7572, 7412, 7280], "Failure": [6592, 6696, 6812, 6668, 6550], "Yield": [5648, 5666, 5724, 5572, 5618]},
        "L3": {"Compressive": [5276, 5326, 5354, 5250, 5300], "Failure": [4748, 4784, 4814, 4726, 4764], "Yield": [4038, 4062, 4100, 3994, 4028]},
        "L4": {"Compressive": [7568, 7574, 7608, 7496, 7580], "Failure": [6816, 6816, 6842, 6744, 6820], "Yield": [5702, 5636, 5616, 5574, 5624]},
        "L5": {"Compressive": [4788, 4726, 4814, 4816, 4750], "Failure": [4544, 4486, 4570, 4572, 4510], "Yield": [4520, 4478, 4532, 4554, 4500]},
        
        "M1": {"Compressive": [6496, 6580, 6536, 6486, 6484], "Failure": [6168, 6248, 6212, 6164, 6160], "Yield": [5532, 5574, 5540, 5520, 5580]},
        "M2": {"Compressive": [7598, 7540, 7738, 7678, 7632], "Failure": [7220, 7162, 7350, 7306, 7248], "Yield": [6312, 6336, 6412, 6302, 6368]},
        "M3": {"Compressive": [5608, 5736, 5722, 5630, 5482], "Failure": [5326, 5448, 5426, 5348, 5206], "Yield": [4896, 5010, 5014, 4910, 4886]},
        "M4": {"Compressive": [6510, 6650, 6630, 6532, 6426], "Failure": [6184, 6318, 6296, 6204, 6102], "Yield": [5360, 5520, 5508, 5390, 5332]},
        "M5": {"Compressive": [6528, 6554, 6568, 6616, 6570], "Failure": [6200, 6228, 6238, 6288, 6240], "Yield": [5604, 5628, 5572, 5652, 5646]},
        
        "H1": {"Compressive": [6500, 6594, 6526, 6456, 6526], "Failure": [6170, 6272, 6194, 6128, 6196], "Yield": [5378, 5442, 5436, 5322, 5368]},
        "H2": {"Compressive": [6632, 6636, 6584, 6572, 6572], "Failure": [6300, 6302, 6254, 6240, 6236], "Yield": [5606, 5534, 5626, 5552, 5552]},
        "H3": {"Compressive": [6568, 6558, 6516, 6564, 6410], "Failure": [6236, 6230, 6184, 6238, 6088], "Yield": [5358, 5350, 5244, 5372, 5266]},
        "H4": {"Compressive": [6474, 6522, 6458, 6536, 6388], "Failure": [6146, 6194, 6140, 6206, 6066], "Yield": [5310, 5294, 5252, 5326, 5218]},
        "H5": {"Compressive": [6112, 6258, 6298, 6292, 6386], "Failure": [5802, 5946, 5978, 5978, 6066], "Yield": [5016, 5122, 5116, 5066, 5208]}
    }

    rho_map = {}
    for name in source_datasets_raw.keys():
        if name.startswith("L"): rho_map[name] = 0.1
        elif name.startswith("M"): rho_map[name] = 0.5
        elif name.startswith("H"): rho_map[name] = 0.9

    print(f"{'Property':<15} | {'True Value':<10} | {'Pred Value':<10} | {'Test MAPE':<10} | {'Optimization':<15}")
    print("-" * 75)

    total_mape = 0

    for prop in ["Compressive", "Failure", "Yield"]:
       
        prop_sources = {k: np.array(v[prop]) for k, v in source_datasets_raw.items()}
        
        optimizer = MultiSourceBayesianTransfer(
            target_train=target_data[prop]["train"],
            source_datasets=prop_sources,
            rho_map=rho_map
        )
      
        best_weights, min_loo_error = optimizer.optimize_weights_de()
        
        final_pred = optimizer.predict(best_weights)
        true_val = target_data[prop]["test"]
        test_error = mape(true_val, final_pred)
        total_mape += test_error
        
        sorted_indices = np.argsort(best_weights)[::-1]
        top_sources = [f"{optimizer.source_names[i]}({best_weights[i]:.2f})" for i in sorted_indices[:3]]
        top_sources_str = ", ".join(top_sources)
        print(f"{prop:<15} | {true_val:<10} | {final_pred:<10.2f} | {test_error:<9.2f}% | Top: {top_sources_str}")

    print("-" * 75)
    print(f"Average Test MAPE (Multi-Source DE Optimized): {total_mape / 3:.2f}%")
